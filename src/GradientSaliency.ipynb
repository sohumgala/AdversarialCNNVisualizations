{"cells":[{"cell_type":"markdown","metadata":{"id":"lAiCgB-XcoOU"},"source":["SETUP:\n","1. Download input_images from our drive folder and upload them to colab's runtime files"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3FmIlyk_qftf"},"outputs":[],"source":["import torch\n","import os\n","import torch.nn as nn\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","from torch import optim\n","from torch.autograd import Variable\n","\n","import numpy as np\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","from matplotlib import cm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vbdL02UEzjjB"},"outputs":[],"source":["# setup (visualization library only supports CPU)\n","device = torch.device('cpu')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ankM2vZYBIjt"},"outputs":[],"source":["# CNN Visualizer code: (https://github.com/utkuozbulak/pytorch-cnn-visualizations)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xLd3jmlSrCWa"},"outputs":[],"source":["# misc_functions.py\n","\n","\"\"\"\n","Created on Thu Oct 21 11:09:09 2017\n","@author: Utku Ozbulak - github.com/utkuozbulak\n","\"\"\"\n","import os\n","import copy\n","import numpy as np\n","from PIL import Image\n","import matplotlib.cm as mpl_color_map\n","from matplotlib.colors import ListedColormap\n","from matplotlib import pyplot as plt\n","\n","import torch\n","from torch.autograd import Variable\n","from torchvision import models\n","\n","\n","def convert_to_grayscale(im_as_arr):\n","    \"\"\"\n","        Converts 3d image to grayscale\n","    Args:\n","        im_as_arr (numpy arr): RGB image with shape (D,W,H)\n","    returns:\n","        grayscale_im (numpy_arr): Grayscale image with shape (1,W,D)\n","    \"\"\"\n","    grayscale_im = np.sum(np.abs(im_as_arr), axis=0)\n","    im_max = np.percentile(grayscale_im, 99)\n","    im_min = np.min(grayscale_im)\n","    grayscale_im = (np.clip((grayscale_im - im_min) / (im_max - im_min), 0, 1))\n","    grayscale_im = np.expand_dims(grayscale_im, axis=0)\n","    return grayscale_im\n","\n","\n","def save_gradient_images(gradient, file_name):\n","    \"\"\"\n","        Exports the original gradient image\n","    Args:\n","        gradient (np arr): Numpy array of the gradient with shape (3, 224, 224)\n","        file_name (str): File name to be exported\n","    \"\"\"\n","    if not os.path.exists('../results'):\n","        os.makedirs('../results')\n","    # Normalize\n","    gradient = gradient - gradient.min()\n","    gradient /= gradient.max()\n","    # Save image\n","    path_to_file = os.path.join('../results', file_name + '.png')\n","    save_image(gradient, path_to_file)\n","\n","\n","def save_class_activation_images(org_img, activation_map, file_name):\n","    \"\"\"\n","        Saves cam activation map and activation map on the original image\n","    Args:\n","        org_img (PIL img): Original image\n","        activation_map (numpy arr): Activation map (grayscale) 0-255\n","        file_name (str): File name of the exported image\n","    \"\"\"\n","    if not os.path.exists('../results'):\n","        os.makedirs('../results')\n","    # Grayscale activation map\n","    heatmap, heatmap_on_image = apply_colormap_on_image(org_img, activation_map, 'hsv')\n","    # Save colored heatmap\n","    path_to_file = os.path.join('../results', file_name+'_Cam_Heatmap.png')\n","    save_image(heatmap, path_to_file)\n","    # Save heatmap on iamge\n","    path_to_file = os.path.join('../results', file_name+'_Cam_On_Image.png')\n","    save_image(heatmap_on_image, path_to_file)\n","    # SAve grayscale heatmap\n","    path_to_file = os.path.join('../results', file_name+'_Cam_Grayscale.png')\n","    save_image(activation_map, path_to_file)\n","\n","\n","def apply_colormap_on_image(org_im, activation, colormap_name):\n","    \"\"\"\n","        Apply heatmap on image\n","    Args:\n","        org_img (PIL img): Original image\n","        activation_map (numpy arr): Activation map (grayscale) 0-255\n","        colormap_name (str): Name of the colormap\n","    \"\"\"\n","    # Get colormap\n","    color_map = mpl_color_map.get_cmap(colormap_name)\n","    no_trans_heatmap = color_map(activation)\n","    # Change alpha channel in colormap to make sure original image is displayed\n","    heatmap = copy.copy(no_trans_heatmap)\n","    heatmap[:, :, 3] = 0.4\n","    heatmap = Image.fromarray((heatmap*255).astype(np.uint8))\n","    no_trans_heatmap = Image.fromarray((no_trans_heatmap*255).astype(np.uint8))\n","\n","    # Apply heatmap on image\n","    heatmap_on_image = Image.new(\"RGBA\", org_im.size)\n","    heatmap_on_image = Image.alpha_composite(heatmap_on_image, org_im.convert('RGBA'))\n","    heatmap_on_image = Image.alpha_composite(heatmap_on_image, heatmap)\n","    return no_trans_heatmap, heatmap_on_image\n","\n","\n","def apply_heatmap(R, sx, sy):\n","    \"\"\"\n","        Heatmap code stolen from https://git.tu-berlin.de/gmontavon/lrp-tutorial\n","        This is (so far) only used for LRP\n","    \"\"\"\n","    b = 10*((np.abs(R)**3.0).mean()**(1.0/3))\n","    my_cmap = plt.cm.seismic(np.arange(plt.cm.seismic.N))\n","    my_cmap[:, 0:3] *= 0.85\n","    my_cmap = ListedColormap(my_cmap)\n","    plt.figure(figsize=(sx, sy))\n","    plt.subplots_adjust(left=0, right=1, bottom=0, top=1)\n","    plt.axis('off')\n","    heatmap = plt.imshow(R, cmap=my_cmap, vmin=-b, vmax=b, interpolation='nearest')\n","    return heatmap\n","    # plt.show()\n","\n","\n","def format_np_output(np_arr):\n","    \"\"\"\n","        This is a (kind of) bandaid fix to streamline saving procedure.\n","        It converts all the outputs to the same format which is 3xWxH\n","        with using sucecssive if clauses.\n","    Args:\n","        im_as_arr (Numpy array): Matrix of shape 1xWxH or WxH or 3xWxH\n","    \"\"\"\n","    # Phase/Case 1: The np arr only has 2 dimensions\n","    # Result: Add a dimension at the beginning\n","    if len(np_arr.shape) == 2:\n","        np_arr = np.expand_dims(np_arr, axis=0)\n","    # Phase/Case 2: Np arr has only 1 channel (assuming first dim is channel)\n","    # Result: Repeat first channel and convert 1xWxH to 3xWxH\n","    if np_arr.shape[0] == 1:\n","        np_arr = np.repeat(np_arr, 3, axis=0)\n","    # Phase/Case 3: Np arr is of shape 3xWxH\n","    # Result: Convert it to WxHx3 in order to make it saveable by PIL\n","    if np_arr.shape[0] == 3:\n","        np_arr = np_arr.transpose(1, 2, 0)\n","    # Phase/Case 4: NP arr is normalized between 0-1\n","    # Result: Multiply with 255 and change type to make it saveable by PIL\n","    if np.max(np_arr) <= 1:\n","        np_arr = (np_arr*255).astype(np.uint8)\n","    return np_arr\n","\n","\n","def save_image(im, path):\n","    \"\"\"\n","        Saves a numpy matrix or PIL image as an image\n","    Args:\n","        im_as_arr (Numpy array): Matrix of shape DxWxH\n","        path (str): Path to the image\n","    \"\"\"\n","    if isinstance(im, (np.ndarray, np.generic)):\n","        im = format_np_output(im)\n","        im = Image.fromarray(im)\n","    im.save(path)\n","\n","\n","def preprocess_image(pil_im, resize_im=True):\n","    \"\"\"\n","        Processes image for CNNs\n","    Args:\n","        PIL_img (PIL_img): PIL Image or numpy array to process\n","        resize_im (bool): Resize to 224 or not\n","    returns:\n","        im_as_var (torch variable): Variable that contains processed float tensor\n","    \"\"\"\n","    # Mean and std list for channels (Imagenet)\n","    mean = [0.485, 0.456, 0.406]\n","    std = [0.229, 0.224, 0.225]\n","\n","    # Ensure or transform incoming image to PIL image\n","    if type(pil_im) != Image.Image:\n","        try:\n","            pil_im = Image.fromarray(pil_im)\n","        except Exception as e:\n","            print(\"could not transform PIL_img to a PIL Image object. Please check input.\")\n","\n","    # Resize image\n","    if resize_im:\n","        pil_im = pil_im.resize((224, 224), Image.ANTIALIAS)\n","\n","    im_as_arr = np.float32(pil_im)\n","    im_as_arr = im_as_arr.transpose(2, 0, 1)  # Convert array to D,W,H\n","    # Normalize the channels\n","    for channel, _ in enumerate(im_as_arr):\n","        im_as_arr[channel] /= 255\n","        im_as_arr[channel] -= mean[channel]\n","        im_as_arr[channel] /= std[channel]\n","    # Convert to float tensor\n","    im_as_ten = torch.from_numpy(im_as_arr).float()\n","    # Add one more channel to the beginning. Tensor shape = 1,3,224,224\n","    im_as_ten.unsqueeze_(0)\n","    # Convert to Pytorch variable\n","    im_as_var = Variable(im_as_ten, requires_grad=True)\n","    return im_as_var\n","\n","\n","def recreate_image(im_as_var):\n","    \"\"\"\n","        Recreates images from a torch variable, sort of reverse preprocessing\n","    Args:\n","        im_as_var (torch variable): Image to recreate\n","    returns:\n","        recreated_im (numpy arr): Recreated image in array\n","    \"\"\"\n","    reverse_mean = [-0.485, -0.456, -0.406]\n","    reverse_std = [1/0.229, 1/0.224, 1/0.225]\n","    recreated_im = copy.copy(im_as_var.data.numpy()[0])\n","    for c in range(3):\n","        recreated_im[c] /= reverse_std[c]\n","        recreated_im[c] -= reverse_mean[c]\n","    recreated_im[recreated_im > 1] = 1\n","    recreated_im[recreated_im < 0] = 0\n","    recreated_im = np.round(recreated_im * 255)\n","\n","    recreated_im = np.uint8(recreated_im).transpose(1, 2, 0)\n","    return recreated_im\n","\n","\n","def get_positive_negative_saliency(gradient):\n","    \"\"\"\n","        Generates positive and negative saliency maps based on the gradient\n","    Args:\n","        gradient (numpy arr): Gradient of the operation to visualize\n","    returns:\n","        pos_saliency ( )\n","    \"\"\"\n","    pos_saliency = (np.maximum(0, gradient) / gradient.max())\n","    neg_saliency = (np.maximum(0, -gradient) / -gradient.min())\n","    return pos_saliency, neg_saliency\n","\n","\n","def get_example_params(example_index):\n","    \"\"\"\n","        Gets used variables for almost all visualizations, like the image, model etc.\n","    Args:\n","        example_index (int): Image id to use from examples\n","    returns:\n","        original_image (numpy arr): Original image read from the file\n","        prep_img (numpy_arr): Processed image\n","        target_class (int): Target class for the image\n","        file_name_to_export (string): File name to export the visualizations\n","        pretrained_model(Pytorch model): Model to use for the operations\n","    \"\"\"\n","\n","    \n","    # Pick one of the examples\n","    example_list = (('input_images/805.jpeg', 805),\n","                    ('input_images/image_254.jpeg', 245),\n","                    ('input_images/ostrich_hammer_vgg16.png', 587),\n","                    ('input_images/ostrich_labcoat_vgg16.png', 617),\n","                    ('input_images/ostrich_acornsquash_vgg16.png', 941),\n","                    ('input_images/ostrich_goldfinch_vgg16.png', 11),\n","                    ('input_images/corn_hammer_vgg16.png', 587),\n","                    ('input_images/corn_labcoat_vgg16.png', 617),\n","                    ('input_images/corn_acornsquash_vgg16.png', 941),\n","                    ('input_images/corn_goldfinch_vgg16.png', 11))\n","    img_path = example_list[example_index][0]\n","    target_class = example_list[example_index][1]\n","    file_name_to_export = img_path[img_path.rfind('/')+1:img_path.rfind('.')]\n","    # Read image\n","    original_image = Image.open(img_path).convert('RGB')\n","    # Process image\n","    prep_img = preprocess_image(original_image)\n","    # Define model\n","    pretrained_model = models.alexnet(pretrained=True)\n","    return (original_image,\n","            prep_img,\n","            target_class,\n","            file_name_to_export,\n","            pretrained_model)"]},{"cell_type":"code","source":["#@title create class specific adversarial image\n","# Create adversarial image\n","\n","import os\n","import numpy as np\n","\n","import torch\n","from torch.optim import SGD\n","from torchvision import models\n","\n","class ClassSpecificImageGeneration():\n","    \"\"\"\n","        Produces an image that maximizes a certain class with gradient ascent\n","    \"\"\"\n","    def __init__(self, model, starting_image, target_class):\n","        self.mean = [-0.485, -0.456, -0.406]\n","        self.std = [1/0.229, 1/0.224, 1/0.225]\n","        self.model = model\n","        self.model.eval()\n","        self.target_class = target_class\n","        # Generate a random image\n","        self.created_image = recreate_image(starting_image)\n","        # Create the folder to export images if not exists\n","        if not os.path.exists('../generated/class_'+str(self.target_class)):\n","            os.makedirs('../generated/class_'+str(self.target_class))\n","\n","    def generate(self, iterations=150):\n","        \"\"\"Generates class specific image\n","        Keyword Arguments:\n","            iterations {int} -- Total iterations for gradient ascent (default: {150})\n","        Returns:\n","            np.ndarray -- Final maximally activated class image\n","        \"\"\"\n","        initial_learning_rate = 6\n","        for i in range(1, iterations):\n","            # Process image and return variable\n","            self.processed_image = preprocess_image(self.created_image, False)\n","\n","            # Define optimizer for the image\n","            optimizer = SGD([self.processed_image], lr=initial_learning_rate)\n","            # Forward\n","            output = self.model(self.processed_image)\n","            # Target specific class\n","            class_loss = -output[0, self.target_class]\n","\n","            if i % 10 == 0 or i == iterations-1:\n","                print('Iteration:', str(i), 'Loss',\n","                      \"{0:.2f}\".format(class_loss.data.numpy()))\n","            # Zero grads\n","            self.model.zero_grad()\n","            # Backward\n","            class_loss.backward()\n","            # Update image\n","            optimizer.step()\n","            # Recreate image\n","            self.created_image = recreate_image(self.processed_image)\n","            # if i % 10 == 0 or i == iterations-1:\n","            #     # Save image\n","            #     im_path = '../generated/class_'+str(self.target_class)+'/c_'+str(self.target_class)+'_'+'iter_'+str(i)+'.png'\n","            #     save_image(self.created_image, im_path)\n","\n","        return self.processed_image"],"metadata":{"id":"3hjXRMUSkvjw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3uhstHIIW9RV"},"source":["Next Steps (discussed between Sohum and Mohit):\n","1. Pick a pretrainted model and dataset\n","2. Find a correctly classified input\n","3. Generate a perturbed input [example](https://medium.com/@ml.at.berkeley/tricking-neural-networks-create-your-own-adversarial-examples-a61eb7620fd8)\n","4. Provide visualizations of these inputs as they go through the pretrained model\n","5. Report findings about where they diverge, where the outputs look very different\n","6. Organize results in a way that communicates this understanding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dxRA1V-5dft6"},"outputs":[],"source":["target_example = 0  # Corn\n","(corn_image, corn_tensor, corn_class, file_name_to_export, pretrained_model) = get_example_params(target_example)\n","\n","#target_example = 1 # Ostrich\n","#(ostrich_image, ostrich_tensor, ostrich_class, file_name_to_export, pretrained_model) = get_example_params(target_example)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZrkWYwBQvuf0","executionInfo":{"status":"ok","timestamp":1670041488976,"user_tz":300,"elapsed":137,"user":{"displayName":"Akash Vemulapalli","userId":"08774496744157678973"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c5ef7c89-7231-4f88-84b9-b7306cf22622"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["805"]},"metadata":{},"execution_count":25}],"source":["with torch.no_grad():\n","    output = pretrained_model(corn_tensor)\n","torch.argmax(output[0]).item()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5284,"status":"ok","timestamp":1670041596979,"user":{"displayName":"Akash Vemulapalli","userId":"08774496744157678973"},"user_tz":300},"id":"ikvCC128zc6l","outputId":"bb8ffa20-0c3f-4e00-bbd8-80c2d775aef5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Iteration: 4 Loss -20.54\n","932\n","Iteration: 4 Loss -32.36\n","112\n","Iteration: 4 Loss -17.68\n","526\n","Iteration: 4 Loss -17.02\n","499\n","updating: input_images/ (stored 0%)\n","updating: input_images/805.jpeg (stored 0%)\n","updating: input_images/image_254.jpeg (deflated 1%)\n","updating: input_images/corn_parachute_alexnet.png (deflated 0%)\n","updating: input_images/corn_analog_clock_alexnet.png (deflated 0%)\n","updating: input_images/corn_submarine_alexnet.png (deflated 0%)\n","updating: input_images/.ipynb_checkpoints/ (stored 0%)\n","updating: input_images/corn_sandal_alexnet.png (deflated 0%)\n","updating: input_images/corn_bulletproof_vest_alexnet.png (deflated 0%)\n","updating: input_images/corn_pretzel_alexnet.png (deflated 0%)\n","updating: input_images/corn_conch_alexnet.png (deflated 0%)\n","updating: input_images/corn_flute_alexnet.png (deflated 0%)\n","  adding: input_images/corn_cleaver_alexnet.png (deflated 0%)\n","  adding: input_images/corn_desk_alexnet.png (deflated 0%)\n"]}],"source":["adversarial_classes = [(932, \"pretzel\"), (112, \"conch\"), (526, \"desk\"), (499, \"cleaver\")]\n","\n","for adversarial_class, label in adversarial_classes:\n","\n","# adversarial_class = 56\n","# label = \"kingsnake\"\n","\n","  csig = ClassSpecificImageGeneration(pretrained_model, corn_tensor, adversarial_class)\n","  output_tensor = csig.generate(iterations = 5)\n","  with torch.no_grad():\n","    classification = pretrained_model(output_tensor)\n","\n","  classification = torch.argmax(classification[0]).item()\n","  print(classification)\n","\n","  output_np = recreate_image(output_tensor)\n","  output_image = Image.fromarray(output_np)\n","  save_image(output_image, \"input_images/corn_\" + label + \"_alexnet.png\")\n","\n","!zip -r adversarial_images.zip input_images/"]},{"cell_type":"code","source":["for adversarial_class, label in adversarial_classes:\n","\n","  csig = ClassSpecificImageGeneration(pretrained_model, ostrich_tensor, adversarial_class)\n","  output_tensor = csig.generate(iterations = 5)\n","  with torch.no_grad():\n","    classification = pretrained_model(output_tensor)\n","\n","  classification = torch.argmax(classification[0]).item()\n","  print(classification)\n","\n","  output_np = recreate_image(output_tensor)\n","  output_image = Image.fromarray(output_np)\n","  save_image(output_image, \"input_images/ostrich_\" + label + \"_alexnet.png\")\n","\n","!zip -r adversarial_images.zip input_images/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r8H5nhhZlBZ3","executionInfo":{"status":"ok","timestamp":1670040053785,"user_tz":300,"elapsed":5432,"user":{"displayName":"Akash Vemulapalli","userId":"08774496744157678973"}},"outputId":"54890a1f-7bfb-4135-f8de-679697ce1ecf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Iteration: 4 Loss -19.80\n","695\n","Iteration: 4 Loss -25.87\n","617\n","Iteration: 4 Loss -23.35\n","941\n","Iteration: 4 Loss -11.84\n","11\n","updating: input_images/ (stored 0%)\n","updating: input_images/corn_labcoat_alexnet.png (deflated 0%)\n","updating: input_images/805.jpeg (stored 0%)\n","updating: input_images/image_254.jpeg (deflated 1%)\n","updating: input_images/corn_goldfinch_alexnet.png (deflated 0%)\n","updating: input_images/corn_hammer_alexnet.png (deflated 0%)\n","updating: input_images/corn_acornsquash_alexnet.png (deflated 0%)\n","  adding: input_images/ostrich_acornsquash_alexnet.png (deflated 0%)\n","  adding: input_images/ostrich_labcoat_alexnet.png (deflated 0%)\n","  adding: input_images/ostrich_goldfinch_alexnet.png (deflated 0%)\n","  adding: input_images/ostrich_hammer_alexnet.png (deflated 0%)\n"]}]},{"cell_type":"markdown","source":["## Saliency Maps\n"],"metadata":{"id":"4fkZ1on50Zbf"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":383},"id":"RDxdGbfjT0_R","executionInfo":{"status":"error","timestamp":1669917909255,"user_tz":300,"elapsed":161,"user":{"displayName":"Akash Vemulapalli","userId":"08774496744157678973"}},"outputId":"0da9d74e-decc-4f5a-f657-7998913eae56"},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-5636c0062eb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompile_saliency_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \"\"\"\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'theano'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["import theano\n","import theano.tensor as T\n","\n","def compile_saliency_function(net):\n","    \"\"\"\n","    Compiles a function to compute the saliency maps and predicted classes\n","    for a given minibatch of input images.\n","    \"\"\"\n","    inp = net['input'].input_var\n","    outp = lasagne.layers.get_output(net['fc8'], deterministic=True)\n","    max_outp = T.max(outp, axis=1)\n","    saliency = theano.grad(max_outp.sum(), wrt=inp)\n","    max_class = T.argmax(outp, axis=1)\n","    return theano.function([inp], [saliency, max_class])\n","\n","def show_images(img_original, saliency, max_class, title):\n","    # get out the first map and class from the mini-batch\n","    saliency = saliency[0]\n","    max_class = max_class[0]\n","    # convert saliency from BGR to RGB, and from c01 to 01c\n","    saliency = saliency[::-1].transpose(1, 2, 0)\n","    # plot the original image and the three saliency map variants\n","    plt.figure(figsize=(10, 10), facecolor='w')\n","    plt.suptitle(\"Class: \" + classes[max_class] + \". Saliency: \" + title)\n","    plt.subplot(2, 2, 1)\n","    plt.title('input')\n","    plt.imshow(img_original)\n","    plt.subplot(2, 2, 2)\n","    plt.title('abs. saliency')\n","    plt.imshow(np.abs(saliency).max(axis=-1), cmap='gray')\n","    plt.subplot(2, 2, 3)\n","    plt.title('pos. saliency')\n","    plt.imshow((np.maximum(0, saliency) / saliency.max()))\n","    plt.subplot(2, 2, 4)\n","    plt.title('neg. saliency')\n","    plt.imshow((np.maximum(0, -saliency) / -saliency.min()))\n","    plt.show()\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%config InlineBackend.figure_format = 'jpeg'\n","%matplotlib inline\n","import urllib\n","import io\n","import skimage.transform\n","\n","def prepare_image(url):\n","    ext = url.rsplit('.', 1)[1]\n","    img = plt.imread(io.BytesIO(urllib.urlopen(url).read()), ext)\n","    # Resize so smallest dim = 256, preserving aspect ratio\n","    h, w, _ = img.shape\n","    if h < w:\n","        img = skimage.transform.resize(img, (256, w*256/h), preserve_range=True)\n","    else:\n","        img = skimage.transform.resize(img, (h*256/w, 256), preserve_range=True)\n","    # Central crop to 224x224\n","    h, w, _ = img.shape\n","    img = img[h//2-112:h//2+112, w//2-112:w//2+112]\n","    # Remember this, it's a single RGB image suitable for plt.imshow()\n","    img_original = img.astype('uint8')\n","    # Shuffle axes from 01c to c01\n","    img = img.transpose(2, 0, 1)\n","    # Convert from RGB to BGR\n","    img = img[::-1]\n","    # Subtract mean pixel value\n","    img = img - mean_pixel[:, np.newaxis, np.newaxis]\n","    # Return the original and the prepared image (as a batch of a single item)\n","    return img_original, lasagne.utils.floatX(img[np.newaxis])"]},{"cell_type":"markdown","source":["#VGG"],"metadata":{"id":"HRqPM0Qv4LMC"}},{"cell_type":"code","source":["pretrained_model = models.vgg16(pretrained=True)\n","pretrained_model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KY3h-wYN4MaB","executionInfo":{"status":"ok","timestamp":1669783982291,"user_tz":300,"elapsed":2457,"user":{"displayName":"rutvik marakana","userId":"06312633065460319635"}},"outputId":"58e217ce-c25e-4c89-8f44-e720646c8cfd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n","  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"output_type":"execute_result","data":{"text/plain":["VGG(\n","  (features): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): ReLU(inplace=True)\n","    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (3): ReLU(inplace=True)\n","    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (6): ReLU(inplace=True)\n","    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (8): ReLU(inplace=True)\n","    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (11): ReLU(inplace=True)\n","    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (13): ReLU(inplace=True)\n","    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (15): ReLU(inplace=True)\n","    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (18): ReLU(inplace=True)\n","    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (20): ReLU(inplace=True)\n","    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (22): ReLU(inplace=True)\n","    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (25): ReLU(inplace=True)\n","    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (27): ReLU(inplace=True)\n","    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (29): ReLU(inplace=True)\n","    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n","  (classifier): Sequential(\n","    (0): Linear(in_features=25088, out_features=4096, bias=True)\n","    (1): ReLU(inplace=True)\n","    (2): Dropout(p=0.5, inplace=False)\n","    (3): Linear(in_features=4096, out_features=4096, bias=True)\n","    (4): ReLU(inplace=True)\n","    (5): Dropout(p=0.5, inplace=False)\n","    (6): Linear(in_features=4096, out_features=1000, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":67}]},{"cell_type":"code","source":["adversarial_classes = [(587, \"hammer\"), (617, \"labcoat\"), (941, \"acornsquash\"), (11, \"goldfinch\")]\n","\n","for adversarial_class, label in adversarial_classes:\n","\n","# adversarial_class = 56\n","# label = \"kingsnake\"\n","\n","  csig = ClassSpecificImageGeneration(pretrained_model, corn_tensor, adversarial_class)\n","  output_tensor = csig.generate(iterations = 5)\n","  with torch.no_grad():\n","    classification = pretrained_model(output_tensor)\n","\n","  classification = torch.argmax(classification[0]).item()\n","  print(classification)\n","\n","  output_np = recreate_image(output_tensor)\n","  output_image = Image.fromarray(output_np)\n","  save_image(output_image, \"input_images/corn_\" + label + \"_vgg16.png\")\n","\n","!zip -r adversarial_images.zip input_images/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FIIUuJir4NPC","executionInfo":{"status":"ok","timestamp":1669784270648,"user_tz":300,"elapsed":33482,"user":{"displayName":"rutvik marakana","userId":"06312633065460319635"}},"outputId":"578ad4e2-0dca-4989-b7ee-28440d4915ab"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Iteration: 4 Loss -12.21\n","462\n","Iteration: 4 Loss -10.13\n","998\n","Iteration: 4 Loss -20.27\n","941\n","Iteration: 4 Loss -15.26\n","318\n","updating: input_images/ (stored 0%)\n","updating: input_images/corn.JPEG (deflated 0%)\n","updating: input_images/ostrich.JPEG (deflated 0%)\n","updating: input_images/.ipynb_checkpoints/ (stored 0%)\n","  adding: input_images/corn_labcoat_vgg16.png (deflated 0%)\n","  adding: input_images/corn_goldfinch_vgg16.png (deflated 0%)\n","  adding: input_images/corn_hammer_vgg16.png (deflated 0%)\n","  adding: input_images/corn_acornsquash_vgg16.png (deflated 0%)\n"]}]},{"cell_type":"code","source":["for adversarial_class, label in adversarial_classes:\n","\n","  csig = ClassSpecificImageGeneration(pretrained_model, ostrich_tensor, adversarial_class)\n","  output_tensor = csig.generate(iterations = 5)\n","  with torch.no_grad():\n","    classification = pretrained_model(output_tensor)\n","\n","  classification = torch.argmax(classification[0]).item()\n","  print(classification)\n","\n","  output_np = recreate_image(output_tensor)\n","  output_image = Image.fromarray(output_np)\n","  save_image(output_image, \"input_images/ostrich_\" + label + \"_vgg16.png\")\n","\n","!zip -r adversarial_images.zip input_images/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qHVy9jB-4fqS","executionInfo":{"status":"ok","timestamp":1669784333244,"user_tz":300,"elapsed":35356,"user":{"displayName":"rutvik marakana","userId":"06312633065460319635"}},"outputId":"2d7d385d-0afa-4690-deb6-09e6a727ecdd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Iteration: 4 Loss -32.94\n","587\n","Iteration: 4 Loss -10.58\n","617\n","Iteration: 4 Loss -26.32\n","941\n","Iteration: 4 Loss -14.66\n","84\n","updating: input_images/ (stored 0%)\n","updating: input_images/corn.JPEG (deflated 0%)\n","updating: input_images/ostrich.JPEG (deflated 0%)\n","updating: input_images/.ipynb_checkpoints/ (stored 0%)\n","updating: input_images/corn_labcoat_vgg16.png (deflated 0%)\n","updating: input_images/corn_goldfinch_vgg16.png (deflated 0%)\n","updating: input_images/corn_hammer_vgg16.png (deflated 0%)\n","updating: input_images/corn_acornsquash_vgg16.png (deflated 0%)\n","  adding: input_images/ostrich_acornsquash_vgg16.png (deflated 0%)\n","  adding: input_images/ostrich_goldfinch_vgg16.png (deflated 0%)\n","  adding: input_images/ostrich_hammer_vgg16.png (deflated 0%)\n","  adding: input_images/ostrich_labcoat_vgg16.png (deflated 0%)\n"]}]},{"cell_type":"markdown","source":["## Saliency Maps"],"metadata":{"id":"G-xgmd072SjB"}},{"cell_type":"code","source":["url = 'http://farm5.static.flickr.com/4064/4334173592_145856d89b.jpg'\n","img_original, img = prepare_image(url)\n","\n","saliency_fn = compile_saliency_function(pretrained_model)\n","saliency, max_class = saliency_fn(img)\n","show_images(img_original, saliency, max_class, \"default gradient\")"],"metadata":{"id":"whPptTdX25Va"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Jz1Svd3rCBvD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install tf-keras-vis\n"],"metadata":{"id":"LaDsEEVb6Xsb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#dependencies\n","import shutil\n","import os\n","import random\n","import glob\n","import matplotlib.pyplot as plt\n","from matplotlib.pyplot import figure, imshow, axis\n","from matplotlib.image import imread\n","import tensorflow as tf\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","\n","#Unecessary since we already have our model trained\n","\n","\"\"\"\n","#extracting the images\n","filename = \"train.zip\"\n","shutil.unpack_archive(filename, \"images\")\n","\n","#storing cat and dog files in lists\n","cat_files = glob.glob(\"images/train/cat*.jpg\")\n","dog_files = glob.glob(\"images/train/dog*.jpg\")\n","\n","#make training and test dir\n","os.mkdir(\"images/test\")\n","os.mkdir(\"images/train/dog\")\n","os.mkdir(\"images/train/cat\")\n","os.mkdir(\"images/test/dog\")\n","os.mkdir(\"images/test/cat\")\n","\n","#moving training data and testing data to the appropriate folders\n","training_dir = \"images/train\"\n","testing_dir = \"images/test\"\n","\n","training_dog = \"images/train/dog\"\n","training_cat = \"images/train/cat\"\n","\n","testing_dog = \"images/test/dog\"\n","testing_cat = \"images/test/cat\"\n","\n","#only using 1000 and 100 per class out of 12500 images for training and testing, respectively  \n","train_dog_files = random.sample(dog_files, 1000)\n","train_cat_files = random.sample(cat_files, 1000)\n","test_dog_files = [file for file in random.sample(dog_files, 100) if file not in train_dog_files]\n","test_cat_files = [file for file in random.sample(cat_files, 100) if file not in train_cat_files]\n","\n","for file in train_dog_files:\n","    shutil.move(file, \"images/train/dog\")\n","\n","for file in train_cat_files:\n","    shutil.move(file, \"images/train/cat\")\n","\n","for file in test_dog_files:\n","    shutil.move(file, \"images/test/dog\")\n","\n","for file in test_cat_files:\n","    shutil.move(file, \"images/test/cat\")\n","\n","print(f'total training dog images: {len(os.listdir(training_dog))}')\n","print(f'total training cat images: {len(os.listdir(training_cat))}')\n","print(f'total validation dog images: {len(os.listdir(testing_dog))}')\n","print(f'total validation cat images: {len(os.listdir(testing_cat))}')\n","\n","\n","\n","# All images will be rescaled by 1./255 with some augmentation applied to training images\n","train_datagen = ImageDataGenerator(rescale = 1./255.,\n","                                   rotation_range = 40,\n","                                   width_shift_range = 0.2,\n","                                   height_shift_range = 0.2,\n","                                   shear_range = 0.2,\n","                                   zoom_range = 0.2,\n","                                   horizontal_flip = True)\n","validation_datagen = ImageDataGenerator(rescale=1/255)\n","\n","# Flow training images in batches of 128 using train_datagen generator\n","train_generator = train_datagen.flow_from_directory(\n","        training_dir,  # This is the source directory for training images\n","        target_size=(300, 300),  # All images will be resized to 300x300\n","        batch_size=128,\n","        # Since you use binary_crossentropy loss, you need binary labels\n","        class_mode='binary')\n","\n","# Flow validation images in batches of 128 using validation_datagen generator\n","validation_generator = validation_datagen.flow_from_directory(\n","        testing_dir,  # This is the source directory for validation images\n","        target_size=(300, 300),  # All images will be resized to 300x300\n","        batch_size=32,\n","        # Since you use binary_crossentropy loss, you need binary labels\n","        class_mode='binary')\n","\n","#model architecture\n","model = tf.keras.models.Sequential([\n","    # Note the input shape is the desired size of the image 300x300 with 3 bytes color\n","    # This is the first convolution\n","    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(300, 300, 3)),\n","    tf.keras.layers.MaxPooling2D(2, 2),\n","    # The second convolution\n","    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n","    tf.keras.layers.MaxPooling2D(2,2),\n","    # The third convolution\n","    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n","    tf.keras.layers.MaxPooling2D(2,2),\n","    # The fourth convolution\n","    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n","    tf.keras.layers.MaxPooling2D(2,2),\n","    # The fifth convolution\n","    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n","    tf.keras.layers.MaxPooling2D(2,2),\n","    # Flatten the results to feed into a DNN\n","    tf.keras.layers.Flatten(),\n","    # 512 neuron hidden layer\n","    tf.keras.layers.Dense(512, activation='relu'),\n","    # Only 1 output neuron. \n","    # It will contain a value from 0-1 where 0 for 1 class ('dog') and 1 for the other ('cat')\n","    tf.keras.layers.Dense(1, activation='sigmoid')\n","])\n","\n","#model compilation\n","model.compile(loss='binary_crossentropy',\n","              optimizer=Adam(learning_rate=0.001),\n","              metrics=['accuracy'])\n","\n","#model training\n","history = model.fit(\n","      train_generator,\n","      epochs=10,\n","      verbose=1,\n","      validation_data = validation_generator)\n","\n","\"\"\""],"metadata":{"id":"_ULeUxNlB2le"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.image import load_img, img_to_array\n","from tensorflow.keras.preprocessing import image\n","from tensorflow.keras import backend as K\n","from tf_keras_vis.saliency import Saliency\n","from tf_keras_vis.utils import normalize\n","from vis.utils import utils\n","\n","#loading the image and converting it to numpy array\n","#img = tf.keras.preprocessing.image.load_img('images/test/dog/'+ random.sample(os.listdir(testing_dog), 1)[0],target_size=(300,300))\n","img = tf.keras.preprocessing.image.load_img('hen.png', target_size=(300,300))\n","\n","x = img_to_array(img)  # Numpy array with shape (300, 300, 3)\n","x = x.reshape((1,) + x.shape)  # Numpy array with shape (1, 300, 300, 3)\n","\n","# swap last layer with linear layer \n","layer_idx = utils.find_layer_idx(model, model.layers[-1].name)\n","model.layers[-1].activation = tf.keras.activations.linear\n","model = utils.apply_modifications(model)\n","\n","from tf_keras_vis.utils.scores import CategoricalScore\n","score = CategoricalScore([0])\n","\n","#Create Saliency object\n","saliency = Saliency(model, clone=False)\n","\n","subplot_args = {\n","   'nrows': 1,\n","   'ncols': 1,\n","   'figsize': (5, 4),\n","   'subplot_kw': {'xticks': [], 'yticks': []}\n","}\n","\n","# Generate saliency map\n","saliency_map = saliency(score, x, smooth_samples=20, smooth_noise=0.2)\n","saliency_map = normalize(saliency_map)\n","\n","f, ax = plt.subplots(**subplot_args)\n","ax.imshow(saliency_map[0], cmap='Reds')\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"QnkimGBzB53r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install flashtorch torch==1.5.0 torchvision==0.6.0 -U\n","\n"],"metadata":{"id":"0iqicjShLdw4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import torchvision.models as models\n","\n","from flashtorch.utils import apply_transforms, load_image\n","from flashtorch.saliency import Backprop\n","\n","image = load_image('hen.png')\n","\n","plt.imshow(image)\n","plt.title('Original image')\n","plt.axis('off');"],"metadata":{"id":"mx25yjJuLh4v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = models.alexnet(pretrained=True)\n","backprop = Backprop(model)\n","\n"],"metadata":{"id":"GbRY23AfLohK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","img = apply_transforms(image)\n","\n","target_class = 8\n","\n","#8 for hen and 56 for the snake\n","\n","\n","backprop.visualize(img, target_class, guided=True, use_gpu=True)"],"metadata":{"id":"5AVaAOLNLqz1"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}